{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('env_cpb': conda)",
      "metadata": {
        "interpreter": {
          "hash": "6e562eb495edbd98b4705d13b4dc4f4058fca01d43484b094130bc2fb66d6fcd"
        }
      }
    },
    "colab": {
      "name": "bert_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d9f817a3dd84a1380434a88f9f64247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d60d26fa91f54603a9db25e5ff705a26",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bdba8f32cdf842dfba2b9f0821e2a915",
              "IPY_MODEL_5545136024b94db88a76f99562056a54"
            ]
          }
        },
        "d60d26fa91f54603a9db25e5ff705a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdba8f32cdf842dfba2b9f0821e2a915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_071839f7bdd644ebaba3d6861b530bad",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c6fd74deda541ef8b187c7e291f01ed"
          }
        },
        "5545136024b94db88a76f99562056a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a15f76ad920479caa907995c7c91afc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.21MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_479addd093b44fc39b375e53ba3a1bb7"
          }
        },
        "071839f7bdd644ebaba3d6861b530bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c6fd74deda541ef8b187c7e291f01ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a15f76ad920479caa907995c7c91afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "479addd093b44fc39b375e53ba3a1bb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63e06d11b71841c1a79c52e58f19a3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2214cd0a1ea4cae9a6fcf20ad5ee9b3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92548d70d7434bbf80a821db00c2ecd3",
              "IPY_MODEL_0d52f21ce03249ed87a066019d9bdafe"
            ]
          }
        },
        "a2214cd0a1ea4cae9a6fcf20ad5ee9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92548d70d7434bbf80a821db00c2ecd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58c4f10cd72940a39f45a572b44c715d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7538a5bd6854d8bb493590bfed5e57d"
          }
        },
        "0d52f21ce03249ed87a066019d9bdafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9fecbec401ca4cd2ad3094c77d1f7812",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.02kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1b8c405bc4c41bd802bd2365ecee9ee"
          }
        },
        "58c4f10cd72940a39f45a572b44c715d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7538a5bd6854d8bb493590bfed5e57d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fecbec401ca4cd2ad3094c77d1f7812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1b8c405bc4c41bd802bd2365ecee9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d850d4b9f39b40f797798f85d2a6996f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_355abf9d8c454e0db873d721c55c2a91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e542e7039cb342d5979c7fb16fd5c275",
              "IPY_MODEL_99b226f42cdb44aaa2aa1aabc322632f"
            ]
          }
        },
        "355abf9d8c454e0db873d721c55c2a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e542e7039cb342d5979c7fb16fd5c275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2badb1d6483045b5a817237d938a7771",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c58a5b004734e7da5dd133f2983d5e9"
          }
        },
        "99b226f42cdb44aaa2aa1aabc322632f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a0302641a1a497bba7ae4b029e5f33d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:28&lt;00:00, 15.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8caeba5efaa4e7d837f4924bfb9581a"
          }
        },
        "2badb1d6483045b5a817237d938a7771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c58a5b004734e7da5dd133f2983d5e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a0302641a1a497bba7ae4b029e5f33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8caeba5efaa4e7d837f4924bfb9581a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6B_gq2KP8hy",
        "outputId": "1a58b0c7-e90c-4565-94b6-428477dca58c"
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "from torch import cuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=d88feed43d0e6305977f15d5f6994b46144246eb9b14618685b952b89172ac2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "_l4fmkEsP8h4",
        "outputId": "ee62d11a-302e-4c14-a9b3-c47b3011c32c"
      },
      "source": [
        "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data.csv\", index_col=False)\n",
        "df_copy = df[[\"text\", \"super_strategy_label\"]].copy()\n",
        "df_copy.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>super_strategy_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The PressTV references in Wikipedia's \"Turkey-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you have time, can you correct the titles?</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I had really hoped to get some more opinions o...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I modified it a bit, possibly vote to reopen?</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well the only plausible explanation was that y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  super_strategy_label\n",
              "0  The PressTV references in Wikipedia's \"Turkey-...                     1\n",
              "1      If you have time, can you correct the titles?                     3\n",
              "2  I had really hoped to get some more opinions o...                     6\n",
              "3      I modified it a bit, possibly vote to reopen?                     4\n",
              "4  Well the only plausible explanation was that y...                     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LubvY9fVP8h5",
        "outputId": "772700a0-141c-4b36-846e-2d19793d7c98"
      },
      "source": [
        "# Clean the text \n",
        "def normalise_text(text):\n",
        "    text = text.strip()\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.replace(r\"\\#\",\"\") # replaces hashtags\n",
        "    text = text.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
        "    text = text.replace(r\"@\",\"\")\n",
        "    text = text.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
        "    text = text.replace(\"\\s{2,}\", \" \")\n",
        "    text = text.replace(r\"\\#\",\"\")\n",
        "    return text\n",
        "df_copy.text = df_copy.text.apply(normalise_text)\n",
        "df_copy.columns = [\"text\", \"label\"]\n",
        "df_copy.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5094 entries, 0 to 5093\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    5094 non-null   object\n",
            " 1   label   5094 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 79.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F8JOUmaP8h5",
        "outputId": "e3b66735-e93c-4e75-bb79-827b342b1f29"
      },
      "source": [
        "# keep 1/4 0 label\n",
        "df_1 = df_copy.loc[df_copy.label > 0] \n",
        "df_0 = df_copy.loc[df_copy.label == 0]\n",
        "df_0 = df_0.reset_index()\n",
        "df_0 = df_0.sample(frac=0.25, replace=True, random_state=1)\n",
        "train_df = pd.concat([df_1, df_0], ignore_index=True)\n",
        "train_df = train_df.sample(frac=1)\n",
        "train_df = train_df[[\"text\", \"label\"]]\n",
        "train_df.to_csv(\"clean_dataset.csv\", index=False)\n",
        "print(train_df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text  label\n",
            "652   so then \"good luck with your upcoming defense\"...      1\n",
            "334                             sorry, i thought i had.      6\n",
            "524   try the electronic gadgets se:  http://area51....      4\n",
            "1197    ok i've created the following test page: <url>.      6\n",
            "530           have you asked any other iranian members?      4\n",
            "...                                                 ...    ...\n",
            "1783  i do not know what you mean by \"important link...      1\n",
            "2077               thanks for adding that ref to <url>.      7\n",
            "902   also you talk about \"similarity\" - but are you...      1\n",
            "817   before you get into the run-time of the algori...      4\n",
            "2336  why don't use settings > application settings ...      2\n",
            "\n",
            "[3562 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERTkcGF2P8h6",
        "outputId": "308d5a74-99df-44d3-9b1b-8300f1e8d75b"
      },
      "source": [
        "# find max length\n",
        "def get_text_len(text):\n",
        "    return len(text.split(\" \"))\n",
        "max_len = max(train_df.text.apply(get_text_len))\n",
        "max_len"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BN1zPyVP8h6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1d9f817a3dd84a1380434a88f9f64247",
            "d60d26fa91f54603a9db25e5ff705a26",
            "bdba8f32cdf842dfba2b9f0821e2a915",
            "5545136024b94db88a76f99562056a54",
            "071839f7bdd644ebaba3d6861b530bad",
            "1c6fd74deda541ef8b187c7e291f01ed",
            "5a15f76ad920479caa907995c7c91afc",
            "479addd093b44fc39b375e53ba3a1bb7"
          ]
        },
        "outputId": "01f947ce-24af-4d90-b9bf-d809b08de5d1"
      },
      "source": [
        "MAX_LEN = 100\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-05\n",
        "CLASS_SIZE = 8\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9f817a3dd84a1380434a88f9f64247",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hYQPZVXP8h7"
      },
      "source": [
        "class PDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.data.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.label[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2sTGd9AP8h7",
        "outputId": "6c5b17d1-b16e-4d49-d29e-a75b605992df"
      },
      "source": [
        "train_size = 0.8\n",
        "train_dataset = train_df.sample(frac=train_size, random_state=200)\n",
        "test_dataset= train_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(train_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = PDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = PDataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3562, 2)\n",
            "TRAIN Dataset: (2850, 2)\n",
            "TEST Dataset: (712, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ed7lZm5P8h7"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63e06d11b71841c1a79c52e58f19a3df",
            "a2214cd0a1ea4cae9a6fcf20ad5ee9b3",
            "92548d70d7434bbf80a821db00c2ecd3",
            "0d52f21ce03249ed87a066019d9bdafe",
            "58c4f10cd72940a39f45a572b44c715d",
            "f7538a5bd6854d8bb493590bfed5e57d",
            "9fecbec401ca4cd2ad3094c77d1f7812",
            "b1b8c405bc4c41bd802bd2365ecee9ee",
            "d850d4b9f39b40f797798f85d2a6996f",
            "355abf9d8c454e0db873d721c55c2a91",
            "e542e7039cb342d5979c7fb16fd5c275",
            "99b226f42cdb44aaa2aa1aabc322632f",
            "2badb1d6483045b5a817237d938a7771",
            "2c58a5b004734e7da5dd133f2983d5e9",
            "0a0302641a1a497bba7ae4b029e5f33d",
            "d8caeba5efaa4e7d837f4924bfb9581a"
          ]
        },
        "id": "bjuBrlLEP8h8",
        "outputId": "cce1ac88-57bc-4aa6-9a7f-2bdfe8f61d14"
      },
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = BertModel.from_pretrained(\"bert-base-uncased\",return_dict=False)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, CLASS_SIZE)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63e06d11b71841c1a79c52e58f19a3df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d850d4b9f39b40f797798f85d2a6996f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m55aVCAmP8h8"
      },
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJju8YnJP8h8"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WijjEnVPP8h9"
      },
      "source": [
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _ % 1000 == 0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct * 100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 1000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 1000 steps: {accu_step}%\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct * 100)/nb_tr_examples}%')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}%\")\n",
        "    print(\"===========================\")\n",
        "\n",
        "    return epoch_loss, epoch_accu\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY-F21cIP8h9",
        "outputId": "1302098a-6290-40ff-8e16-0edb7e36317c"
      },
      "source": [
        "from pandas import DataFrame\n",
        "epochs_loss_history = []\n",
        "epochs_accu_history = []\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss, epoch_accu = train(epoch)\n",
        "    epochs_loss_history.append(epoch_loss)\n",
        "    epochs_accu_history.append(epoch_accu)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 1000 steps: 2.0852317810058594\n",
            "Training Accuracy per 1000 steps: 25.0%\n",
            "The Total Accuracy for Epoch 0: 47.6140350877193%\n",
            "Training Loss Epoch: 1.5141387188900783\n",
            "Training Accuracy Epoch: 47.6140350877193%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 1.0849411487579346\n",
            "Training Accuracy per 1000 steps: 68.75%\n",
            "The Total Accuracy for Epoch 1: 69.6140350877193%\n",
            "Training Loss Epoch: 0.9098246903392856\n",
            "Training Accuracy Epoch: 69.6140350877193%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.42389774322509766\n",
            "Training Accuracy per 1000 steps: 87.5%\n",
            "The Total Accuracy for Epoch 2: 81.05263157894737%\n",
            "Training Loss Epoch: 0.6097130330914226\n",
            "Training Accuracy Epoch: 81.05263157894737%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.732241690158844\n",
            "Training Accuracy per 1000 steps: 68.75%\n",
            "The Total Accuracy for Epoch 3: 87.54385964912281%\n",
            "Training Loss Epoch: 0.4331059909982388\n",
            "Training Accuracy Epoch: 87.54385964912281%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.3732973337173462\n",
            "Training Accuracy per 1000 steps: 87.5%\n",
            "The Total Accuracy for Epoch 4: 90.98245614035088%\n",
            "Training Loss Epoch: 0.30956820783585143\n",
            "Training Accuracy Epoch: 90.98245614035088%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.11669053882360458\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 5: 94.0%\n",
            "Training Loss Epoch: 0.20423757992476724\n",
            "Training Accuracy Epoch: 94.0%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0516321025788784\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 6: 96.52631578947368%\n",
            "Training Loss Epoch: 0.1311113605868883\n",
            "Training Accuracy Epoch: 96.52631578947368%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.14368467032909393\n",
            "Training Accuracy per 1000 steps: 93.75%\n",
            "The Total Accuracy for Epoch 7: 97.96491228070175%\n",
            "Training Loss Epoch: 0.09613422184204423\n",
            "Training Accuracy Epoch: 97.96491228070175%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.11503627896308899\n",
            "Training Accuracy per 1000 steps: 93.75%\n",
            "The Total Accuracy for Epoch 8: 98.87719298245614%\n",
            "Training Loss Epoch: 0.06098779038572944\n",
            "Training Accuracy Epoch: 98.87719298245614%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0712951272726059\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 9: 99.33333333333333%\n",
            "Training Loss Epoch: 0.042088683954747026\n",
            "Training Accuracy Epoch: 99.33333333333333%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.007487118244171143\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 10: 98.94736842105263%\n",
            "Training Loss Epoch: 0.04891397775926071\n",
            "Training Accuracy Epoch: 98.94736842105263%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.1941864788532257\n",
            "Training Accuracy per 1000 steps: 93.75%\n",
            "The Total Accuracy for Epoch 11: 99.50877192982456%\n",
            "Training Loss Epoch: 0.029678382762504858\n",
            "Training Accuracy Epoch: 99.50877192982456%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0066705187782645226\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 12: 99.64912280701755%\n",
            "Training Loss Epoch: 0.02593533641674332\n",
            "Training Accuracy Epoch: 99.64912280701755%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0490344874560833\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 13: 99.71929824561404%\n",
            "Training Loss Epoch: 0.016744966393690452\n",
            "Training Accuracy Epoch: 99.71929824561404%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0037292346823960543\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 14: 99.85964912280701%\n",
            "Training Loss Epoch: 0.013889547213703893\n",
            "Training Accuracy Epoch: 99.85964912280701%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.004820577800273895\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 15: 99.89473684210526%\n",
            "Training Loss Epoch: 0.011249676117850415\n",
            "Training Accuracy Epoch: 99.89473684210526%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0034473564010113478\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 16: 99.6842105263158%\n",
            "Training Loss Epoch: 0.013226599982086381\n",
            "Training Accuracy Epoch: 99.6842105263158%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0036720065400004387\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 17: 99.9298245614035%\n",
            "Training Loss Epoch: 0.006238934230813809\n",
            "Training Accuracy Epoch: 99.9298245614035%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0024366076104342937\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 18: 99.64912280701755%\n",
            "Training Loss Epoch: 0.012771280058373453\n",
            "Training Accuracy Epoch: 99.64912280701755%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.012734225951135159\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 19: 99.78947368421052%\n",
            "Training Loss Epoch: 0.009848616808501994\n",
            "Training Accuracy Epoch: 99.78947368421052%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.007150473538786173\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 20: 100.0%\n",
            "Training Loss Epoch: 0.0034343713410614923\n",
            "Training Accuracy Epoch: 100.0%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0016313489759340882\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 21: 100.0%\n",
            "Training Loss Epoch: 0.002757750228489619\n",
            "Training Accuracy Epoch: 100.0%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.002782734576612711\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 22: 100.0%\n",
            "Training Loss Epoch: 0.002380067241305288\n",
            "Training Accuracy Epoch: 100.0%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.003457062877714634\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 23: 99.96491228070175%\n",
            "Training Loss Epoch: 0.002595058529489471\n",
            "Training Accuracy Epoch: 99.96491228070175%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0010665401350706816\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 24: 99.89473684210526%\n",
            "Training Loss Epoch: 0.0034293589522338902\n",
            "Training Accuracy Epoch: 99.89473684210526%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.022572150453925133\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 25: 99.12280701754386%\n",
            "Training Loss Epoch: 0.033899482100286295\n",
            "Training Accuracy Epoch: 99.12280701754386%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.00155758170876652\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 26: 99.43859649122807%\n",
            "Training Loss Epoch: 0.019694795307518612\n",
            "Training Accuracy Epoch: 99.43859649122807%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.004103611223399639\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 27: 99.71929824561404%\n",
            "Training Loss Epoch: 0.01223945413298603\n",
            "Training Accuracy Epoch: 99.71929824561404%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0007618701783940196\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 28: 99.54385964912281%\n",
            "Training Loss Epoch: 0.014362839223913244\n",
            "Training Accuracy Epoch: 99.54385964912281%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.019346559420228004\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 29: 99.6140350877193%\n",
            "Training Loss Epoch: 0.015570104808947228\n",
            "Training Accuracy Epoch: 99.6140350877193%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0016796254785731435\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 30: 99.6842105263158%\n",
            "Training Loss Epoch: 0.010901776947570163\n",
            "Training Accuracy Epoch: 99.6842105263158%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.02395707182586193\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 31: 99.78947368421052%\n",
            "Training Loss Epoch: 0.006111279952693627\n",
            "Training Accuracy Epoch: 99.78947368421052%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0008118779514916241\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 32: 99.96491228070175%\n",
            "Training Loss Epoch: 0.0020209372325867996\n",
            "Training Accuracy Epoch: 99.96491228070175%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0008533247746527195\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 33: 99.78947368421052%\n",
            "Training Loss Epoch: 0.008030952712262111\n",
            "Training Accuracy Epoch: 99.78947368421052%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0010880752233788371\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 34: 99.9298245614035%\n",
            "Training Loss Epoch: 0.0025358308794746934\n",
            "Training Accuracy Epoch: 99.9298245614035%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0011727670207619667\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 35: 99.85964912280701%\n",
            "Training Loss Epoch: 0.005684402253278453\n",
            "Training Accuracy Epoch: 99.85964912280701%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.00046475493581965566\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 36: 99.89473684210526%\n",
            "Training Loss Epoch: 0.004068912580699072\n",
            "Training Accuracy Epoch: 99.89473684210526%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0006044756737537682\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 37: 99.9298245614035%\n",
            "Training Loss Epoch: 0.0029117523756934404\n",
            "Training Accuracy Epoch: 99.9298245614035%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.002015270758420229\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 38: 99.96491228070175%\n",
            "Training Loss Epoch: 0.001985575856644837\n",
            "Training Accuracy Epoch: 99.96491228070175%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.00046884670155122876\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 39: 100.0%\n",
            "Training Loss Epoch: 0.0006816806001770804\n",
            "Training Accuracy Epoch: 100.0%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0005303844227455556\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 40: 99.89473684210526%\n",
            "Training Loss Epoch: 0.0019744470055077182\n",
            "Training Accuracy Epoch: 99.89473684210526%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.000374401337467134\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 41: 99.82456140350877%\n",
            "Training Loss Epoch: 0.006936690100864117\n",
            "Training Accuracy Epoch: 99.82456140350877%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0013197326334193349\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 42: 99.82456140350877%\n",
            "Training Loss Epoch: 0.003963303026436261\n",
            "Training Accuracy Epoch: 99.82456140350877%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.08127348124980927\n",
            "Training Accuracy per 1000 steps: 93.75%\n",
            "The Total Accuracy for Epoch 43: 99.40350877192982%\n",
            "Training Loss Epoch: 0.02011626684189808\n",
            "Training Accuracy Epoch: 99.40350877192982%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.000814239087048918\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 44: 99.85964912280701%\n",
            "Training Loss Epoch: 0.0037682621276257143\n",
            "Training Accuracy Epoch: 99.85964912280701%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0005061805713921785\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 45: 99.9298245614035%\n",
            "Training Loss Epoch: 0.002460917651584351\n",
            "Training Accuracy Epoch: 99.9298245614035%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.00029342187917791307\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 46: 99.78947368421052%\n",
            "Training Loss Epoch: 0.006416748223131049\n",
            "Training Accuracy Epoch: 99.78947368421052%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0009810825577005744\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 47: 99.6842105263158%\n",
            "Training Loss Epoch: 0.008240982147168457\n",
            "Training Accuracy Epoch: 99.6842105263158%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.00028638524236157537\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 48: 99.96491228070175%\n",
            "Training Loss Epoch: 0.002163886592733835\n",
            "Training Accuracy Epoch: 99.96491228070175%\n",
            "===========================\n",
            "Training Loss per 1000 steps: 0.0002969002234749496\n",
            "Training Accuracy per 1000 steps: 100.0%\n",
            "The Total Accuracy for Epoch 49: 100.0%\n",
            "Training Loss Epoch: 0.0005460288797540319\n",
            "Training Accuracy Epoch: 100.0%\n",
            "===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em33mbwY4Wlj"
      },
      "source": [
        "df = DataFrame(epochs_loss_history, columns=['bert_epochs_loss_history'])\n",
        "df[\"bert_epochs_accu_history\"] = epochs_accu_history\n",
        "df.to_csv(\"bert_history.csv\", index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbd8HRhpP8h-"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    tr_loss = 0\n",
        "    n_correct = 0 \n",
        "    n_wrong = 0\n",
        "    total = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    f1 = 0.0\n",
        "    targets_list = []\n",
        "    big_idx_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            targets_list.extend(targets.tolist())\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            big_idx_list.extend(big_idx.tolist())\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%100 == 0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct * 100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}%\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}%\")\n",
        "    f1 = f1_score(targets_list, big_idx_list, average='macro')\n",
        "\n",
        "    return epoch_accu, f1\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIFNl1kNP8h-",
        "outputId": "0905225e-85e8-4b95-a006-cc55e3e3aaaa"
      },
      "source": [
        "acc, f1 = valid(model, testing_loader)\n",
        "print(f\"Accuracy on test data: {acc}\")\n",
        "print(f\"Marco F1 score: {f1}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 100 steps: 1.9695684909820557\n",
            "Validation Accuracy per 100 steps: 62.5%\n",
            "Validation Loss Epoch: 0.765538587715291\n",
            "Validation Accuracy Epoch: 87.92134831460675%\n",
            "Accuracy on test data: 87.92134831460675\n",
            "Marco F1 score: 0.8293951435093849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ARhVseP8h-",
        "outputId": "ab1ca5d8-e87d-430a-d55c-dbe06518ecd7"
      },
      "source": [
        "output_model_file = 'pytorch_bert_model.bin'\n",
        "output_vocab_file = 'vocab_bert_model.bin'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('All files saved')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All files saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "CdaDUkG8P8h-",
        "outputId": "a0cc66d7-be60-4aaf-fdae-7272306ffed0"
      },
      "source": [
        "# human annotated testset\n",
        "df = pd.read_csv(\"testset_human.csv\", index_col=False)\n",
        "df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>if you have time, can you correct the titles?</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i had really hoped to get some more opinions o...</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i modified it a bit, possibly vote to reopen?</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>well the only plausible explanation was that y...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is that what indeed occurred?</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0      if you have time, can you correct the titles?    3.0\n",
              "1  i had really hoped to get some more opinions o...    6.0\n",
              "2      i modified it a bit, possibly vote to reopen?    6.0\n",
              "3  well the only plausible explanation was that y...    1.0\n",
              "4                      is that what indeed occurred?    1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byWKro2-A53x"
      },
      "source": [
        "testing_set = PDataset(df, tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXiVXNm7BGvM",
        "outputId": "dd4a5043-39e1-41df-cf37-73a619bc4715"
      },
      "source": [
        "acc, f1 = valid(model, testing_loader)\n",
        "print(f\"Accuracy on test data: {acc}\")\n",
        "print(f\"Marco F1 score: {f1}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 100 steps: 7.092556476593018\n",
            "Validation Accuracy per 100 steps: 37.5%\n",
            "Validation Loss Epoch: 3.966321740831648\n",
            "Validation Accuracy Epoch: 59.61538461538461%\n",
            "Accuracy on test data: 59.61538461538461\n",
            "Marco F1 score: 0.5790985974809504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn-NUXyVBKFV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}